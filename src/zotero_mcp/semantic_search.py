"""
Semantic search functionality for Zotero MCP.

This module provides semantic search capabilities by integrating ChromaDB
with the existing Zotero client to enable vector-based similarity search
over research libraries.
"""

import json
import os
import sys
import threading
import time
from contextlib import contextmanager
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import logging

from pyzotero import zotero

from .chroma_client import ChromaClient, create_chroma_client
from .client import get_zotero_client
from .utils import format_creators, is_local_mode
from .local_db import LocalZoteroReader

logger = logging.getLogger(__name__)


@contextmanager
def suppress_stdout():
    """Context manager to suppress stdout temporarily."""
    with open(os.devnull, 'w') as devnull:
        old_stdout = sys.stdout
        sys.stdout = devnull
        try:
            yield
        finally:
            sys.stdout = old_stdout


class ZoteroSemanticSearch:
    """Semantic search interface for Zotero libraries using ChromaDB."""

    def __init__(self,
                 chroma_client: ChromaClient | None = None,
                 config_path: str | None = None,
                 db_path: str | None = None):
        """
        Initialize semantic search.

        Args:
            chroma_client: Optional ChromaClient instance
            config_path: Path to configuration file
            db_path: Optional path to Zotero database (overrides config file)
        """
        self.chroma_client = chroma_client or create_chroma_client(config_path)
        self.zotero_client = get_zotero_client()
        self.config_path = config_path
        self.db_path = db_path  # CLI override for Zotero database path

        # Load update configuration
        self.update_config = self._load_update_config()

    def _load_update_config(self) -> dict[str, Any]:
        """Load update configuration from file or use defaults."""
        config = {
            "auto_update": False,
            "update_frequency": "manual",
            "last_update": None,
            "update_days": 7
        }

        if self.config_path and os.path.exists(self.config_path):
            try:
                with open(self.config_path) as f:
                    file_config = json.load(f)
                    config.update(file_config.get("semantic_search", {}).get("update_config", {}))
            except Exception as e:
                logger.warning(f"Error loading update config: {e}")

        return config

    def _save_update_config(self) -> None:
        """Save update configuration to file."""
        if not self.config_path:
            return

        config_dir = Path(self.config_path).parent
        config_dir.mkdir(parents=True, exist_ok=True)

        # Load existing config or create new one
        full_config = {}
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path) as f:
                    full_config = json.load(f)
            except Exception:
                pass

        # Update semantic search config
        if "semantic_search" not in full_config:
            full_config["semantic_search"] = {}

        full_config["semantic_search"]["update_config"] = self.update_config

        try:
            with open(self.config_path, 'w') as f:
                json.dump(full_config, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving update config: {e}")

    def _create_document_text(self, item: dict[str, Any]) -> str:
        """
        Create searchable text from a Zotero item.

        Args:
            item: Zotero item dictionary

        Returns:
            Combined text for embedding
        """
        data = item.get("data", {})

        # Extract key fields for semantic search
        title = data.get("title", "")
        abstract = data.get("abstractNote", "")

        # Format creators as text
        creators = data.get("creators", [])
        creators_text = format_creators(creators)

        # Additional searchable content
        extra_fields = []

        # Publication details
        if publication := data.get("publicationTitle"):
            extra_fields.append(publication)

        # Tags
        if tags := data.get("tags"):
            tag_text = " ".join([tag.get("tag", "") for tag in tags])
            extra_fields.append(tag_text)

        # Note content (if available)
        if note := data.get("note"):
            # Clean HTML from notes
            import re
            note_text = re.sub(r'<[^>]+>', '', note)
            extra_fields.append(note_text)

        # Combine all text fields
        text_parts = [title, creators_text, abstract] + extra_fields
        return " ".join(filter(None, text_parts))

    def _create_metadata(self, item: dict[str, Any]) -> dict[str, Any]:
        """
        Create metadata for a Zotero item.

        Args:
            item: Zotero item dictionary

        Returns:
            Metadata dictionary for ChromaDB
        """
        data = item.get("data", {})

        metadata = {
            "item_key": item.get("key", ""),
            "item_type": data.get("itemType", ""),
            "title": data.get("title", ""),
            "date": data.get("date", ""),
            "date_added": data.get("dateAdded", ""),
            "date_modified": data.get("dateModified", ""),
            "creators": format_creators(data.get("creators", [])),
            "publication": data.get("publicationTitle", ""),
            "url": data.get("url", ""),
            "doi": data.get("DOI", ""),
        }
        # If local fulltext field exists, add markers so we can filter later
        if data.get("fulltext"):
            metadata["has_fulltext"] = True
            if data.get("fulltextSource"):
                metadata["fulltext_source"] = data.get("fulltextSource")

        # Add tags as a single string
        if tags := data.get("tags"):
            metadata["tags"] = " ".join([tag.get("tag", "") for tag in tags])
        else:
            metadata["tags"] = ""

        # Add citation key if available
        extra = data.get("extra", "")
        citation_key = ""
        for line in extra.split("\n"):
            if line.lower().startswith(("citation key:", "citationkey:")):
                citation_key = line.split(":", 1)[1].strip()
                break
        metadata["citation_key"] = citation_key

        return metadata

    def should_update_database(self) -> bool:
        """Check if the database should be updated based on configuration."""
        if not self.update_config.get("auto_update", False):
            return False

        frequency = self.update_config.get("update_frequency", "manual")

        if frequency == "manual":
            return False
        elif frequency == "startup":
            return True
        elif frequency == "daily":
            last_update = self.update_config.get("last_update")
            if not last_update:
                return True

            last_update_date = datetime.fromisoformat(last_update)
            return datetime.now() - last_update_date >= timedelta(days=1)
        elif frequency.startswith("every_"):
            try:
                days = int(frequency.split("_")[1])
                last_update = self.update_config.get("last_update")
                if not last_update:
                    return True

                last_update_date = datetime.fromisoformat(last_update)
                return datetime.now() - last_update_date >= timedelta(days=days)
            except (ValueError, IndexError):
                return False

        return False

    def _update_database_local_fulltext_pipelined(
        self,
        *,
        stats: dict[str, Any],
        limit: int | None,
        force_rebuild: bool,
    ) -> None:
        """Update database by piping local fulltext extraction into embedding/upsert."""
        from concurrent.futures import ProcessPoolExecutor, wait, FIRST_COMPLETED
        import multiprocessing as mp

        from .fulltext_worker import extract_fulltext_for_item, init_fulltext_worker

        # Load per-run config, including extraction limits and db path if provided.
        pdf_max_pages = None
        pipelines: int | None = None
        docling_device: str | None = None
        docling_num_threads: int | None = None
        docling_gpu_ids: list[str] | None = None
        zotero_db_path = self.db_path  # CLI override takes precedence

        # If semantic_search config file exists, prefer its setting.
        try:
            if self.config_path and os.path.exists(self.config_path):
                with open(self.config_path) as _f:
                    _cfg = json.load(_f)
                    semantic_cfg = _cfg.get("semantic_search", {})
                    fulltext_cfg = semantic_cfg.get("fulltext", {}) or {}
                    extraction_cfg = semantic_cfg.get("extraction", {}) or {}

                    def _coerce_pos_int(value: Any) -> int | None:
                        if isinstance(value, int):
                            return value if value > 0 else None
                        if isinstance(value, str) and value.strip():
                            try:
                                parsed = int(value.strip())
                                return parsed if parsed > 0 else None
                            except ValueError:
                                return None
                        return None

                    pdf_max_pages = fulltext_cfg.get("pdf_max_pages")
                    if pdf_max_pages is None:
                        pdf_max_pages = extraction_cfg.get("pdf_max_pages")

                    pipelines = _coerce_pos_int(fulltext_cfg.get("pipelines"))
                    if pipelines is None:
                        pipelines = _coerce_pos_int(extraction_cfg.get("workers"))

                    docling_device = (
                        fulltext_cfg.get("docling_device")
                        if fulltext_cfg.get("docling_device") is not None
                        else extraction_cfg.get("docling_device")
                    )
                    docling_num_threads = (
                        fulltext_cfg.get("docling_num_threads")
                        if fulltext_cfg.get("docling_num_threads") is not None
                        else extraction_cfg.get("docling_num_threads")
                    )
                    docling_gpu_ids = (
                        fulltext_cfg.get("docling_gpu_ids")
                        if fulltext_cfg.get("docling_gpu_ids") is not None
                        else extraction_cfg.get("gpu_ids")
                    )
                    if not zotero_db_path:
                        zotero_db_path = semantic_cfg.get("zotero_db_path")
        except Exception:
            pass

        # Allow env overrides for pipeline concurrency (env > config).
        raw = os.getenv("ZOTERO_PIPELINES")
        if raw:
            try:
                value = int(raw)
                if value > 0:
                    pipelines = value
            except ValueError:
                pass

        raw = os.getenv("ZOTERO_DOCLING_DEVICE")
        if raw and raw.strip():
            docling_device = raw.strip()

        raw = os.getenv("ZOTERO_DOCLING_NUM_THREADS")
        if raw:
            try:
                value = int(raw)
                if value > 0:
                    docling_num_threads = value
            except ValueError:
                pass

        raw = os.getenv("ZOTERO_DOCLING_GPU_IDS")
        if raw:
            docling_gpu_ids = [p.strip() for p in raw.split(",") if p.strip()]
        elif isinstance(docling_gpu_ids, str):
            docling_gpu_ids = [p.strip() for p in docling_gpu_ids.split(",") if p.strip()]
        elif isinstance(docling_gpu_ids, list):
            docling_gpu_ids = [str(p).strip() for p in docling_gpu_ids if str(p).strip()]
        else:
            docling_gpu_ids = None

        # Normalize GPU identifiers for CUDA_VISIBLE_DEVICES pinning: accept either
        # plain ids ("0") or CUDA device strings ("cuda:0").
        if docling_gpu_ids:
            normalized_gpu_ids: list[str] = []
            for raw_id in docling_gpu_ids:
                s = str(raw_id).strip()
                if not s:
                    continue
                lower = s.lower()
                if lower.startswith("cuda:"):
                    suffix = lower.split(":", 1)[1].strip()
                    if suffix.isdigit():
                        s = suffix
                normalized_gpu_ids.append(s)
            docling_gpu_ids = [
                gid for i, gid in enumerate(normalized_gpu_ids)
                if gid and gid not in normalized_gpu_ids[:i]
            ] or None

        # Determine pipeline concurrency.
        effective_pipelines = pipelines or 1
        if effective_pipelines < 1:
            effective_pipelines = 1

        # Helper to build API-compatible item structure for embedding.
        def _to_api_item(item) -> dict[str, Any]:
            api_item = {
                "key": item.key,
                "version": 0,
                "data": {
                    "key": item.key,
                    "itemType": getattr(item, "item_type", None) or "journalArticle",
                    "title": item.title or "",
                    "abstractNote": item.abstract or "",
                    "extra": item.extra or "",
                    "fulltext": getattr(item, "fulltext", None) or "",
                    "fulltextSource": getattr(item, "fulltext_source", None) or "",
                    "dateAdded": item.date_added,
                    "dateModified": item.date_modified,
                    "creators": self._parse_creators_string(item.creators) if item.creators else [],
                },
            }
            if item.notes:
                api_item["data"]["notes"] = item.notes
            return api_item

        # Report progress periodically to keep long runs transparent.
        extracted = 0
        total_to_extract = 0
        skipped_already_indexed = 0
        upgraded_existing = 0
        skipped_no_fulltext = 0
        progress_stop = threading.Event()

        def _write_progress() -> None:
            try:
                sys.stderr.write(
                    "Progress: "
                    f"extracted {extracted}/{total_to_extract} "
                    f"indexed {stats.get('processed_items', 0)} "
                    f"(skipped {skipped_already_indexed} already-indexed, "
                    f"upgraded {upgraded_existing}, "
                    f"no-fulltext {skipped_no_fulltext}, "
                    f"errors {stats.get('errors', 0)})\n"
                )
                sys.stderr.flush()
            except Exception:
                pass

        def _progress_reporter() -> None:
            while not progress_stop.wait(10.0):
                _write_progress()

        # Main workflow: read metadata, decide which items to process, then pipeline extraction -> embed/upsert.
        with suppress_stdout(), LocalZoteroReader(
            db_path=zotero_db_path,
            pdf_max_pages=pdf_max_pages,
            docling_device=docling_device,
            docling_num_threads=docling_num_threads,
        ) as reader:
            sys.stderr.write("Scanning local Zotero database for items...\n")
            local_items = reader.get_items_with_text(limit=limit, include_fulltext=False)
            candidate_count = len(local_items)
            sys.stderr.write(f"Found {candidate_count} candidate items.\n")

            # Optional deduplication: if preprint and journalArticle share a DOI/title, keep journalArticle.
            def norm(s: str | None) -> str | None:
                if not s:
                    return None
                return "".join(s.lower().split())

            key_to_best = {}
            for it in local_items:
                doi_key = ("doi", norm(getattr(it, "doi", None))) if getattr(it, "doi", None) else None
                title_key = ("title", norm(getattr(it, "title", None))) if getattr(it, "title", None) else None

                def consider(k):
                    if not k:
                        return
                    cur = key_to_best.get(k)
                    if cur is None:
                        key_to_best[k] = it
                    else:
                        prefer_types = {"journalArticle": 2, "preprint": 1}
                        cur_score = prefer_types.get(getattr(cur, "item_type", ""), 0)
                        new_score = prefer_types.get(getattr(it, "item_type", ""), 0)
                        if new_score > cur_score:
                            key_to_best[k] = it

                consider(doi_key)
                consider(title_key)

            filtered_items = []
            for it in local_items:
                if getattr(it, "item_type", None) == "preprint":
                    k_doi = ("doi", norm(getattr(it, "doi", None))) if getattr(it, "doi", None) else None
                    k_title = ("title", norm(getattr(it, "title", None))) if getattr(it, "title", None) else None
                    drop = False
                    for k in (k_doi, k_title):
                        if not k:
                            continue
                        best = key_to_best.get(k)
                        if best is not None and best is not it and getattr(best, "item_type", None) == "journalArticle":
                            drop = True
                            break
                    if drop:
                        continue
                filtered_items.append(it)

            local_items = filtered_items

            # Decide which items need extraction/embedding work.
            chroma_client = None if force_rebuild else self.chroma_client
            existing_metadata_by_key: dict[str, dict[str, Any]] = {}
            if chroma_client and not force_rebuild:
                try:
                    sys.stderr.write("Checking existing embeddings...\n")
                except Exception:
                    pass
                keys = [getattr(it, "key", None) for it in local_items]
                keys = [k for k in keys if isinstance(k, str) and k]
                # Chroma can handle batch lookups; chunk to keep requests bounded.
                chunk_size = 1000
                for i in range(0, len(keys), chunk_size):
                    existing_metadata_by_key.update(
                        chroma_client.get_documents_metadata(keys[i:i + chunk_size])
                    )

            items_to_process = []
            for it in local_items:
                meta = existing_metadata_by_key.get(it.key) if existing_metadata_by_key else None
                if meta and meta.get("has_fulltext", False):
                    skipped_already_indexed += 1
                    continue
                items_to_process.append(it)

            total_to_extract = len(items_to_process)
            stats["total_items"] = total_to_extract
            stats["skipped_items"] += skipped_already_indexed

            if total_to_extract != candidate_count:
                sys.stderr.write(
                    f"After filtering/dedup: {len(local_items)} items; {total_to_extract} to process.\n"
                )
            else:
                sys.stderr.write(f"Items to process: {total_to_extract}\n")
            sys.stderr.write("Pipelining fulltext extraction -> embedding/indexing...\n")
            try:
                sys.stderr.write(f"Total items to process: {stats['total_items']}\n")
            except Exception:
                pass

            if total_to_extract == 0:
                return

            # Resolve GPU ids for extraction worker pinning if not explicitly configured.
            gpu_ids = docling_gpu_ids
            if gpu_ids is None and (docling_device or "auto").strip().lower() != "cpu":
                raw_visible = os.getenv("CUDA_VISIBLE_DEVICES")
                if raw_visible:
                    parts = [p.strip() for p in raw_visible.split(",") if p.strip()]
                    if parts:
                        gpu_ids = parts
                if gpu_ids is None:
                    try:
                        import torch

                        if torch.cuda.is_available():
                            count = torch.cuda.device_count()
                            if count > 0:
                                gpu_ids = [str(i) for i in range(count)]
                    except Exception:
                        pass

            if gpu_ids and effective_pipelines > len(gpu_ids):
                try:
                    sys.stderr.write(
                        f"Warning: {effective_pipelines} pipelines over {len(gpu_ids)} GPUs may OOM; consider lowering pipelines or batch sizes.\n"
                    )
                except Exception:
                    pass

            reporter_thread = threading.Thread(target=_progress_reporter, daemon=True)
            reporter_thread.start()

            try:
                embed_batch: list[dict[str, Any]] = []
                embed_batch_size = 50
                flush_interval_s = 5.0
                last_flush = time.monotonic()

                def _flush_embed_batch() -> None:
                    nonlocal last_flush
                    if not embed_batch:
                        return
                    batch_stats = self._process_item_batch(embed_batch, force_rebuild)
                    stats["processed_items"] += batch_stats["processed"]
                    stats["added_items"] += batch_stats["added"]
                    stats["updated_items"] += batch_stats["updated"]
                    stats["skipped_items"] += batch_stats["skipped"]
                    stats["errors"] += batch_stats["errors"]
                    embed_batch.clear()
                    last_flush = time.monotonic()

                if effective_pipelines == 1 or total_to_extract == 1:
                    for it in items_to_process:
                        try:
                            text = reader.extract_fulltext_for_item(it.item_id)
                            if text:
                                if isinstance(text, tuple) and len(text) == 2:
                                    it.fulltext, it.fulltext_source = text[0], text[1]
                                else:
                                    it.fulltext = str(text)
                        except Exception:
                            pass

                        extracted += 1
                        meta = (
                            existing_metadata_by_key.get(it.key)
                            if existing_metadata_by_key
                            else None
                        )
                        if meta is not None and not getattr(it, "fulltext", None):
                            skipped_no_fulltext += 1
                            stats["skipped_items"] += 1
                            continue
                        if meta is not None and getattr(it, "fulltext", None):
                            upgraded_existing += 1
                            stats["updated_items"] += 1
                        embed_batch.append(_to_api_item(it))

                        now = time.monotonic()
                        if len(embed_batch) >= embed_batch_size or (now - last_flush) >= flush_interval_s:
                            _flush_embed_batch()
                else:
                    ctx = mp.get_context("spawn")
                    max_in_flight = max(effective_pipelines * 4, embed_batch_size)
                    it_iter = iter(items_to_process)

                    with ProcessPoolExecutor(
                        max_workers=effective_pipelines,
                        mp_context=ctx,
                        initializer=init_fulltext_worker,
                        initargs=(
                            zotero_db_path,
                            pdf_max_pages,
                            docling_device,
                            docling_num_threads,
                            gpu_ids,
                        ),
                    ) as executor:
                        in_flight: dict[Any, Any] = {}

                        def _submit_more() -> None:
                            while len(in_flight) < max_in_flight:
                                try:
                                    nxt = next(it_iter)
                                except StopIteration:
                                    break
                                fut = executor.submit(extract_fulltext_for_item, nxt.item_id)
                                in_flight[fut] = nxt

                        _submit_more()

                        while in_flight:
                            done, _ = wait(in_flight, return_when=FIRST_COMPLETED)
                            for fut in done:
                                it = in_flight.pop(fut)
                                try:
                                    _, text, source = fut.result()
                                    if text:
                                        it.fulltext = text
                                        it.fulltext_source = source
                                except Exception:
                                    pass

                                extracted += 1
                                meta = (
                                    existing_metadata_by_key.get(it.key)
                                    if existing_metadata_by_key
                                    else None
                                )
                                if meta is not None and not getattr(it, "fulltext", None):
                                    skipped_no_fulltext += 1
                                    stats["skipped_items"] += 1
                                    continue
                                if meta is not None and getattr(it, "fulltext", None):
                                    upgraded_existing += 1
                                    stats["updated_items"] += 1
                                embed_batch.append(_to_api_item(it))

                                now = time.monotonic()
                                if len(embed_batch) >= embed_batch_size or (now - last_flush) >= flush_interval_s:
                                    _flush_embed_batch()

                            _submit_more()

                _flush_embed_batch()

            finally:
                progress_stop.set()
                try:
                    reporter_thread.join(timeout=1.0)
                except Exception:
                    pass
                _write_progress()

    def _parse_creators_string(self, creators_str: str) -> list[dict[str, str]]:
        """
        Parse creators string from local DB into API format.

        Args:
            creators_str: String like "Smith, John; Doe, Jane"

        Returns:
            List of creator objects
        """
        if not creators_str:
            return []

        creators = []
        for creator in creators_str.split(';'):
            creator = creator.strip()
            if not creator:
                continue

            if ',' in creator:
                last, first = creator.split(',', 1)
                creators.append({
                    "creatorType": "author",
                    "firstName": first.strip(),
                    "lastName": last.strip()
                })
            else:
                creators.append({
                    "creatorType": "author",
                    "name": creator
                })

        return creators

    def _get_items_from_api(self, limit: int | None = None) -> list[dict[str, Any]]:
        """
        Get items from Zotero API (original implementation).

        Args:
            limit: Optional limit on number of items

        Returns:
            List of items from API
        """
        logger.info("Fetching items from Zotero API...")

        # Fetch items in batches to handle large libraries
        batch_size = 100
        start = 0
        all_items = []

        while True:
            batch_params = {"start": start, "limit": batch_size}
            if limit and len(all_items) >= limit:
                break

            try:
                items = self.zotero_client.items(**batch_params)
            except Exception as e:
                if "Connection refused" in str(e):
                    error_msg = (
                        "Cannot connect to Zotero local API. Please ensure:\n"
                        "1. Zotero is running\n"
                        "2. Local API is enabled in Zotero Preferences > Advanced > Enable HTTP server\n"
                        "3. The local API port (default 23119) is not blocked"
                    )
                    raise Exception(error_msg) from e
                else:
                    raise Exception(f"Zotero API connection error: {e}") from e
            if not items:
                break

            # Filter out attachments and notes by default
            filtered_items = [
                item for item in items
                if item.get("data", {}).get("itemType") not in ["attachment", "note"]
            ]

            all_items.extend(filtered_items)
            start += batch_size

            if len(items) < batch_size:
                break

        if limit:
            all_items = all_items[:limit]

        logger.info(f"Retrieved {len(all_items)} items from API")
        return all_items

    def update_database(self,
                       force_full_rebuild: bool = False,
                       limit: int | None = None,
                       extract_fulltext: bool = False) -> dict[str, Any]:
        """
        Update the semantic search database with Zotero items.

        Args:
            force_full_rebuild: Whether to rebuild the entire database
            limit: Limit number of items to process (for testing)
            extract_fulltext: Whether to extract fulltext content from local database

        Returns:
            Update statistics
        """
        logger.info("Starting database update...")
        start_time = datetime.now()

        stats = {
            "total_items": 0,
            "processed_items": 0,
            "added_items": 0,
            "updated_items": 0,
            "skipped_items": 0,
            "errors": 0,
            "start_time": start_time.isoformat(),
            "duration": None
        }

        try:
            # Reset collection if force rebuild
            if force_full_rebuild:
                logger.info("Force rebuilding database...")
                self.chroma_client.reset_collection()

            if extract_fulltext and is_local_mode():
                # Local mode: pipeline extraction and embedding so GPUs/CPU can overlap.
                self._update_database_local_fulltext_pipelined(
                    stats=stats,
                    limit=limit,
                    force_rebuild=force_full_rebuild,
                )
            else:
                # Metadata-only update path (API). Fulltext extraction is only supported
                # in local mode via the pipelined fulltext updater.
                all_items = self._get_items_from_api(limit)

                stats["total_items"] = len(all_items)
                logger.info(f"Found {stats['total_items']} items to process")
                # Immediate progress line so users see counts up-front
                try:
                    sys.stderr.write(f"Total items to index: {stats['total_items']}\n")
                except Exception:
                    pass

                # Process items in batches
                batch_size = 50
                # Track next milestone for progress printing (every 10 items)
                next_milestone = 10 if stats["total_items"] >= 10 else stats["total_items"]
                # Count of items seen (including skipped), used for progress milestones
                seen_items = 0
                for i in range(0, len(all_items), batch_size):
                    batch = all_items[i:i + batch_size]
                    batch_stats = self._process_item_batch(batch, force_full_rebuild)

                    stats["processed_items"] += batch_stats["processed"]
                    stats["added_items"] += batch_stats["added"]
                    stats["updated_items"] += batch_stats["updated"]
                    stats["skipped_items"] += batch_stats["skipped"]
                    stats["errors"] += batch_stats["errors"]
                    seen_items += len(batch)

                    logger.info(f"Processed {seen_items}/{stats['total_items']} items (added: {stats['added_items']}, skipped: {stats['skipped_items']})")
                    # Print progress every 10 seen items (even if all are skipped)
                    try:
                        while seen_items >= next_milestone and next_milestone > 0:
                            sys.stderr.write(f"Processed: {next_milestone}/{stats['total_items']} added:{stats['added_items']} skipped:{stats['skipped_items']} errors:{stats['errors']}\n")
                            next_milestone += 10
                            if next_milestone > stats["total_items"]:
                                next_milestone = stats["total_items"]
                                break
                    except Exception:
                        pass

            # Update last update time
            self.update_config["last_update"] = datetime.now().isoformat()
            self._save_update_config()

            end_time = datetime.now()
            stats["duration"] = str(end_time - start_time)
            stats["end_time"] = end_time.isoformat()

            logger.info(f"Database update completed in {stats['duration']}")
            return stats

        except Exception as e:
            logger.error(f"Error updating database: {e}")
            stats["error"] = str(e)
            end_time = datetime.now()
            stats["duration"] = str(end_time - start_time)
            return stats

    def _process_item_batch(self, items: list[dict[str, Any]], force_rebuild: bool = False) -> dict[str, int]:
        """Process a batch of items."""
        stats = {"processed": 0, "added": 0, "updated": 0, "skipped": 0, "errors": 0}

        documents = []
        metadatas = []
        ids = []

        for item in items:
            try:
                item_key = item.get("key", "")
                if not item_key:
                    stats["skipped"] += 1
                    continue

                # Create document text and metadata
                # Prefer fulltext if available, else fall back to structured fields
                fulltext = item.get("data", {}).get("fulltext", "")
                doc_text = fulltext if fulltext.strip() else self._create_document_text(item)
                metadata = self._create_metadata(item)

                if not doc_text.strip():
                    stats["skipped"] += 1
                    continue

                documents.append(doc_text)
                metadatas.append(metadata)
                ids.append(item_key)

                stats["processed"] += 1

            except Exception as e:
                logger.error(f"Error processing item {item.get('key', 'unknown')}: {e}")
                stats["errors"] += 1

        # Add documents to ChromaDB if any
        if documents:
            try:
                self.chroma_client.upsert_documents(documents, metadatas, ids)
                stats["added"] += len(documents)
            except Exception as e:
                logger.error(f"Error adding documents to ChromaDB: {e}")
                stats["errors"] += len(documents)

        return stats

    def search(self,
               query: str,
               limit: int = 10,
               filters: dict[str, Any] | None = None) -> dict[str, Any]:
        """
        Perform semantic search over the Zotero library.

        Args:
            query: Search query text
            limit: Maximum number of results to return
            filters: Optional metadata filters

        Returns:
            Search results with Zotero item details
        """
        try:
            # Perform semantic search
            results = self.chroma_client.search(
                query_texts=[query],
                n_results=limit,
                where=filters
            )

            # Enrich results with full Zotero item data
            enriched_results = self._enrich_search_results(results, query)

            return {
                "query": query,
                "limit": limit,
                "filters": filters,
                "results": enriched_results,
                "total_found": len(enriched_results)
            }

        except Exception as e:
            logger.error(f"Error performing semantic search: {e}")
            return {
                "query": query,
                "limit": limit,
                "filters": filters,
                "results": [],
                "total_found": 0,
                "error": str(e)
            }

    def _enrich_search_results(self, chroma_results: dict[str, Any], query: str) -> list[dict[str, Any]]:
        """Enrich ChromaDB results with full Zotero item data."""
        enriched = []

        if not chroma_results.get("ids") or not chroma_results["ids"][0]:
            return enriched

        ids = chroma_results["ids"][0]
        distances = chroma_results.get("distances", [[]])[0]
        documents = chroma_results.get("documents", [[]])[0]
        metadatas = chroma_results.get("metadatas", [[]])[0]

        for i, item_key in enumerate(ids):
            try:
                # Get full item data from Zotero
                zotero_item = self.zotero_client.item(item_key)

                enriched_result = {
                    "item_key": item_key,
                    "similarity_score": 1 - distances[i] if i < len(distances) else 0,
                    "matched_text": documents[i] if i < len(documents) else "",
                    "metadata": metadatas[i] if i < len(metadatas) else {},
                    "zotero_item": zotero_item,
                    "query": query
                }

                enriched.append(enriched_result)

            except Exception as e:
                logger.error(f"Error enriching result for item {item_key}: {e}")
                # Include basic result even if enrichment fails
                enriched.append({
                    "item_key": item_key,
                    "similarity_score": 1 - distances[i] if i < len(distances) else 0,
                    "matched_text": documents[i] if i < len(documents) else "",
                    "metadata": metadatas[i] if i < len(metadatas) else {},
                    "query": query,
                    "error": f"Could not fetch full item data: {e}"
                })

        return enriched

    def get_database_status(self) -> dict[str, Any]:
        """Get status information about the semantic search database."""
        collection_info = self.chroma_client.get_collection_info()

        return {
            "collection_info": collection_info,
            "update_config": self.update_config,
            "should_update": self.should_update_database(),
            "last_update": self.update_config.get("last_update"),
        }

    def delete_item(self, item_key: str) -> bool:
        """Delete an item from the semantic search database."""
        try:
            self.chroma_client.delete_documents([item_key])
            return True
        except Exception as e:
            logger.error(f"Error deleting item {item_key}: {e}")
            return False


def create_semantic_search(config_path: str | None = None, db_path: str | None = None) -> ZoteroSemanticSearch:
    """
    Create a ZoteroSemanticSearch instance.

    Args:
        config_path: Path to configuration file
        db_path: Optional path to Zotero database (overrides config file)

    Returns:
        Configured ZoteroSemanticSearch instance
    """
    return ZoteroSemanticSearch(config_path=config_path, db_path=db_path)
